\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{kirkpatrick1984optimization}
\citation{fraser1957simulation}
\citation{de1997mimic}
\citation{Castro:2013}
\citation{Castro:2013}
\citation{baluja1997using}
\citation{de1997mimic}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background Information}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Algorithms}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Randomized Hill Climbing (RHC)}{1}{subsubsection.2.1.1}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Simulated Annealing (SA)}{1}{subsubsection.2.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Genetic Algorithms (GA)}{1}{subsubsection.2.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}MIMIC}{1}{subsubsection.2.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Optimization Problems}{1}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Traveling Salesman Problem}{1}{subsubsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Knapsack Problem}{1}{subsubsection.2.2.2}}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Continuous Peaks Problem}{2}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Wisconsin Dataset Overview}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Local Random Search Algorithms}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Introduction}{2}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces This graph displays the three algorithms over 4000 iterations for 30 attributes. As can be seen in the graph, the results seem to converge, but suffer from large fluctuations over some iterations due to their random nature. This is why a lot of the lines seem to have an abundant number of spikes in their path.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:g1}{{1}{2}{This graph displays the three algorithms over 4000 iterations for 30 attributes. As can be seen in the graph, the results seem to converge, but suffer from large fluctuations over some iterations due to their random nature. This is why a lot of the lines seem to have an abundant number of spikes in their path.\relax \relax }{figure.caption.2}{}}
\@LN@col{2}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of Algorithm Performance - The average test time for all algorithms was nearly identical, averaging 0.003s. Due to this, I felt adding an extra column for test time was simply not necessary.\relax }}{2}{table.caption.3}}
\newlabel{tab:t1l}{{1}{2}{Comparison of Algorithm Performance - The average test time for all algorithms was nearly identical, averaging 0.003s. Due to this, I felt adding an extra column for test time was simply not necessary.\relax \relax }{table.caption.3}{}}
\newlabel{tb:t1}{{1}{2}{Comparison of Algorithm Performance - The average test time for all algorithms was nearly identical, averaging 0.003s. Due to this, I felt adding an extra column for test time was simply not necessary.\relax \relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Analysis}{2}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Improvements}{2}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Number of attributes}{2}{subsubsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces This graph displays the three algorithms over 4000 iterations for 20 attributes. Although there are significant minimum errors in this graph, the idea of a minimum for an random search algorithm's error is insignificant because replication would not be possible (without changing the nature of the algorithm). \relax }}{2}{figure.caption.4}}
\newlabel{fig:g2}{{2}{2}{This graph displays the three algorithms over 4000 iterations for 20 attributes. Although there are significant minimum errors in this graph, the idea of a minimum for an random search algorithm's error is insignificant because replication would not be possible (without changing the nature of the algorithm). \relax \relax }{figure.caption.4}{}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This graph displays the three algorithms over 4000 iterations for 10 attributes. The results were very similar to 20 and 30 attributes.\relax }}{3}{figure.caption.5}}
\newlabel{fig:g3}{{3}{3}{This graph displays the three algorithms over 4000 iterations for 10 attributes. The results were very similar to 20 and 30 attributes.\relax \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces This graph compares the average performance of each algorithm over 10 trials. As can be seen, the number of attributes does have a positive effect on all of the algorithms.\relax }}{3}{figure.caption.6}}
\newlabel{fig:g4}{{4}{3}{This graph compares the average performance of each algorithm over 10 trials. As can be seen, the number of attributes does have a positive effect on all of the algorithms.\relax \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Simulated Annealing}{3}{subsubsection.3.3.2}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces This demonstrates the effect of altering cooling for simulated annealing. It is important to note that I only ran one trial per cooling value, so the drops that can be seen in the graph are most likely due to the error that random algorithms presents.\relax }}{3}{figure.caption.7}}
\newlabel{fig:g5}{{5}{3}{This demonstrates the effect of altering cooling for simulated annealing. It is important to note that I only ran one trial per cooling value, so the drops that can be seen in the graph are most likely due to the error that random algorithms presents.\relax \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Genetic Algorithms}{3}{subsubsection.3.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This graph demonstrates that aside from an initial drop in accuracy, a larger population (around 200) has a negligible effect on the accuracy of the algorithm. We take the small changes in accuracy to be strictly due to randomness.\relax }}{3}{figure.caption.8}}
\newlabel{fig:g6}{{6}{3}{This graph demonstrates that aside from an initial drop in accuracy, a larger population (around 200) has a negligible effect on the accuracy of the algorithm. We take the small changes in accuracy to be strictly due to randomness.\relax \relax }{figure.caption.8}{}}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {4}Optimization Problems}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Traveling Salesman Problem (TSP)}{4}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces This figure demonstrates the average 1/distance for GA, SA, and MIMIC. The results were averaged over 15 trials, in order to obtain an accurate representation of their performance.\relax }}{4}{figure.caption.9}}
\newlabel{fig:g7}{{7}{4}{This figure demonstrates the average 1/distance for GA, SA, and MIMIC. The results were averaged over 15 trials, in order to obtain an accurate representation of their performance.\relax \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Knapsack Problem}{4}{subsection.4.2}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces This figure demonstrates the average maximum value obtained for GA, SA, and MIMIC. On average, MIMIC scored 3686, GA scored 3562, and SA scored 3117. \relax }}{4}{figure.caption.10}}
\newlabel{fig:g8}{{8}{4}{This figure demonstrates the average maximum value obtained for GA, SA, and MIMIC. On average, MIMIC scored 3686, GA scored 3562, and SA scored 3117. \relax \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Continuous Peaks Problem}{4}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces This figure demonstrates the average maximum values obtained for the Continuous Peaks problem. It demonstrates the advantages of MIMIC and SA in solving the problem. \relax }}{4}{figure.caption.11}}
\newlabel{fig:g9}{{9}{4}{This figure demonstrates the average maximum values obtained for the Continuous Peaks problem. It demonstrates the advantages of MIMIC and SA in solving the problem. \relax \relax }{figure.caption.11}{}}
\citation{Castro:2013}
\citation{Frank+Asuncion:2010}
\citation{Hall_weka:2010}
\bibstyle{acmsiggraph}
\bibdata{machine_learning_p2}
\bibcite{baluja1997using}{\citename {Baluja and Davies }1997}
\bibcite{Castro:2013}{\citename {Castro }2013}
\bibcite{de1997mimic}{\citename {De\nobreakspace  {}Bonet et\nobreakspace  {}al\unhbox \voidb@x \hbox {.} }1997}
\bibcite{Frank+Asuncion:2010}{\citename {Frank and Asuncion }2010}
\bibcite{fraser1957simulation}{\citename {Fraser }1957}
\bibcite{Hall_weka:2010}{\citename {Hall et\nobreakspace  {}al\unhbox \voidb@x \hbox {.} }2010}
\bibcite{kirkpatrick1984optimization}{\citename {Kirkpatrick }1984}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces This figure was smoothed in order to better illustrate the performance of SA. These 15 trials demonstrate the consistency and precision of the algorithms when run over multiple iterations. The maximum value obtained over all the algorithms and all the trials was 112, which was obtained once by MIMIC, and 4 times by SA.\relax }}{5}{figure.caption.12}}
\newlabel{fig:g10}{{10}{5}{This figure was smoothed in order to better illustrate the performance of SA. These 15 trials demonstrate the consistency and precision of the algorithms when run over multiple iterations. The maximum value obtained over all the algorithms and all the trials was 112, which was obtained once by MIMIC, and 4 times by SA.\relax \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Future Work}{5}{section.6}}
\@LN@col{2}
